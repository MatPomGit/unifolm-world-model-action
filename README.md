# UnifoLM-WMA-0: Framework World-Model-Action (WMA) z Rodziny UnifoLM

<p style="font-size: 1.2em;">
    <a href="https://unigen-x.github.io/unifolm-world-model-action.github.io"><strong>Strona Projektu</strong></a> | 
    <a href="https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c"><strong>Modele</strong></a> |
    <a href="https://huggingface.co/unitreerobotics/datasets"><strong>Zbiory Danych</strong></a> 
  </p>
<div align="center">
  <p align="right">
    <span> ğŸ‡µğŸ‡±Polski </span> | <a href="README_en.md"> ğŸŒEnglish </a> | <a href="README_cn.md"> ğŸ‡¨ğŸ‡³ä¸­æ–‡ </a>
  </p>
</div>

<div align="justify">
    <b>UnifoLM-WMA-0</b> to otwartoÅºrÃ³dÅ‚owa architektura modelu Å›wiata i akcji firmy Unitree, obejmujÄ…ca wiele typÃ³w robotycznych ucieleÅ›nieÅ„ (embodiments), zaprojektowana specjalnie do ogÃ³lnego uczenia robotÃ³w. Jej gÅ‚Ã³wnym komponentem jest model Å›wiata (world-model) zdolny do rozumienia fizycznych interakcji miÄ™dzy robotami a ich otoczeniem. Model ten zapewnia dwie kluczowe funkcje: (a) <b>Silnik Symulacji</b> â€“ dziaÅ‚a jako interaktywny symulator do generowania syntetycznych danych dla uczenia robotÃ³w; (b) <b>Wzmocnienie Polityki</b> â€“ Å‚Ä…czy siÄ™ z gÅ‚owÄ… akcji i poprzez przewidywanie przyszÅ‚ych procesÃ³w interakcji z modelem Å›wiata, dodatkowo optymalizuje wydajnoÅ›Ä‡ podejmowania decyzji przez robota.
</div>

## ğŸ’¡ Co to jest Model Åšwiata (World Model)?
**Dla poczÄ…tkujÄ…cych:** Model Å›wiata to rodzaj sztucznej inteligencji, ktÃ³ra "rozumie" jak dziaÅ‚a otoczenie robota. Podobnie jak czÅ‚owiek potrafi przewidzieÄ‡, Å¼e jeÅ›li pchnie kubek, to ten siÄ™ przewrÃ³ci, model Å›wiata pozwala robotowi przewidywaÄ‡ skutki swoich dziaÅ‚aÅ„ przed ich wykonaniem. To pozwala robotowi planowaÄ‡ lepsze ruchy i uczyÄ‡ siÄ™ z doÅ›wiadczeÅ„.

## ğŸ¦¾ Demonstracje na Rzeczywistych Robotach
| <img src="assets/gifs/real_z1_stackbox.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> | <img src="assets/gifs/real_dual_stackbox.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> |
|:---:|:---:|
| <img src="assets/gifs/real_cleanup_pencils.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> | <img src="assets/gifs/real_g1_pack_camera.gif" style="border:none;box-shadow:none;margin:0;padding:0;" /> |

**Uwaga:** Okno w prawym gÃ³rnym rogu pokazuje przewidywanie modelu Å›wiata dotyczÄ…ce przyszÅ‚ych akcji robota w formie wideo.

## ğŸ”¥ AktualnoÅ›ci

* 22 wrzeÅ›nia 2025: ğŸš€ UdostÄ™pniliÅ›my kod wdroÅ¼eniowy do wspomagania eksperymentÃ³w z robotami [Unitree](https://www.unitree.com/).
* 15 wrzeÅ›nia 2025: ğŸš€ UdostÄ™pniliÅ›my kod treningowy i inferencyjny wraz z wagami modelu [**UnifoLM-WMA-0**](https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c).

## ğŸ“‘ Plan Otwartych Å¹rÃ³deÅ‚
- [x] Trening 
- [x] Inferencja (Wnioskowanie)
- [x] Punkty kontrolne (Checkpoints)
- [x] WdroÅ¼enie (Deployment)

## âš™ï¸ Instalacja

**WyjaÅ›nienie dla poczÄ…tkujÄ…cych:** Ta sekcja przeprowadzi CiÄ™ przez instalacjÄ™ wszystkich potrzebnych narzÄ™dzi i bibliotek. Conda to menedÅ¼er Å›rodowisk, ktÃ³ry pozwala zarzÄ…dzaÄ‡ rÃ³Å¼nymi wersjami Pythona i pakietÃ³w bez konfliktÃ³w.

### Krok 1: Tworzenie Å›rodowiska Conda
Najpierw utworzymy izolowane Å›rodowisko Python z konkretnÄ… wersjÄ…:

```bash
# Tworzy nowe Å›rodowisko o nazwie "unifolm-wma" z Pythonem 3.10.18
conda create -n unifolm-wma python==3.10.18

# Aktywuje utworzone Å›rodowisko (wszystkie kolejne instalacje bÄ™dÄ… w tym Å›rodowisku)
conda activate unifolm-wma
```

### Krok 2: Instalacja zaleÅ¼noÅ›ci systemowych
Teraz zainstalujemy wymagane biblioteki systemowe:

```bash
# Pinocchio - biblioteka do obliczeÅ„ kinematyki i dynamiki robotÃ³w
# UÅ¼ywana do obliczania pozycji i ruchÃ³w ramion robota
conda install pinocchio=3.2.0 -c conda-forge -y

# FFmpeg - narzÄ™dzie do przetwarzania wideo
# Potrzebne do zapisywania i odczytywania filmÃ³w z demonstracji robota
conda install ffmpeg=7.1.1 -c conda-forge
```

### Krok 3: Pobieranie repozytorium
Sklonuj repozytorium wraz z wszystkimi podmoduÅ‚ami (zewnÄ™trznymi bibliotekami):

```bash
# --recurse-submodules zapewnia pobranie rÃ³wnieÅ¼ zaleÅ¼nych repozytoriÃ³w
git clone --recurse-submodules https://github.com/unitreerobotics/unifolm-world-model-action.git
```

**JeÅ›li juÅ¼ pobraÅ‚eÅ› repozytorium wczeÅ›niej bez podmoduÅ‚Ã³w:**
```bash
cd unifolm-world-model-action
# Ta komenda pobiera wszystkie brakujÄ…ce podmoduÅ‚y
git submodule update --init --recursive
```

### Krok 4: Instalacja pakietu gÅ‚Ã³wnego
Zainstaluj gÅ‚Ã³wny pakiet projektu w trybie edytowalnym (pozwala na modyfikacje bez reinstalacji):

```bash
# -e oznacza tryb "editable" - zmiany w kodzie sÄ… od razu widoczne
pip install -e .
```

### Krok 5: Instalacja zewnÄ™trznych zaleÅ¼noÅ›ci
Na koniec zainstaluj pakiet dlimp (zewnÄ™trznÄ… zaleÅ¼noÅ›Ä‡):

```bash
cd external/dlimp
pip install -e .
```

**Gotowe!** Twoje Å›rodowisko jest teraz skonfigurowane i gotowe do pracy.

## ğŸ“‹ Konfiguracja Projektu

Przed rozpoczÄ™ciem treningu lub inferencji, musisz skonfigurowaÄ‡ Å›cieÅ¼ki do modeli i danych. Zobacz szczegÃ³Å‚owy przewodnik:

ğŸ“– **[KONFIGURACJA.md](KONFIGURACJA.md)** - Kompletny przewodnik konfiguracji po polsku
ğŸ“– **[CONFIGURATION.md](CONFIGURATION.md)** - Complete configuration guide in English

Przewodniki zawierajÄ…:
- Wymagane zmiany w plikach konfiguracyjnych
- ZalecanÄ… strukturÄ™ katalogÃ³w
- PrzykÅ‚ady konfiguracji
- WeryfikacjÄ™ poprawnoÅ›ci ustawieÅ„

## ğŸ§° Punkty Kontrolne Modelu (Model Checkpoints)

**WyjaÅ›nienie:** Punkty kontrolne to zapisane wagi modelu po treningu. MoÅ¼esz je wykorzystaÄ‡ bez koniecznoÅ›ci trenowania modelu od zera, co oszczÄ™dza czas i zasoby obliczeniowe.

| Model | Opis | Link|
|---------|-------|------|
|$\text{UnifoLM-WMA-0}_{Base}$| Dostrojony na zbiorze danych [Open-X](https://robotics-transformer-x.github.io/). Dobry punkt startowy do ogÃ³lnych zadaÅ„ robotycznych. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Base)|
|$\text{UnifoLM-WMA-0}_{Dual}$| Dostrojony na piÄ™ciu [zbiorach danych Unitree](https://huggingface.co/collections/unitreerobotics/g1-dex1-datasets-68bae98bf0a26d617f9983ab) w trybie podejmowania decyzji i symulacji. Zoptymalizowany dla robotÃ³w Unitree. | [HuggingFace](https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Dual)|

## ğŸ›¢ï¸ Zbiory Danych (Datasets)

**WyjaÅ›nienie:** Zbiory danych zawierajÄ… nagrania demonstracji robota wykonujÄ…cego rÃ³Å¼ne zadania. Model uczy siÄ™ na tych przykÅ‚adach, jak naleÅ¼y wykonywaÄ‡ poszczegÃ³lne akcje.

W naszych eksperymentach wykorzystujemy nastÄ™pujÄ…ce otwarte zbiory danych:

| ZbiÃ³r Danych | Robot | Zadanie | Link |
|---------|-------|---------|------|
|Z1_StackBox| [Unitree Z1](https://www.unitree.com/z1)| UkÅ‚adanie pudeÅ‚ek | [Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_StackBox_Dataset/tree/v2.1)|
|Z1_DualArm_StackBox|[Unitree Z1](https://www.unitree.com/z1)| UkÅ‚adanie pudeÅ‚ek dwoma ramionami | [Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset/tree/v2.1)|
|Z1_DualArm_StackBox_V2|[Unitree Z1](https://www.unitree.com/z1)| UkÅ‚adanie pudeÅ‚ek v2 | [Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_StackBox_Dataset_V2/tree/v2.1)|
|Z1_DualArm_Cleanup_Pencils|[Unitree Z1](https://www.unitree.com/z1)| SprzÄ…tanie oÅ‚Ã³wkÃ³w | [Huggingface](https://huggingface.co/datasets/unitreerobotics/Z1_Dual_Dex1_CleanupPencils_Dataset/tree/v2.1)|
|G1_Pack_Camera|[Unitree G1](https://www.unitree.com/g1)| Pakowanie kamery do pudeÅ‚ka | [Huggingface](https://huggingface.co/datasets/unitreerobotics/G1_Dex1_MountCameraRedGripper_Dataset/tree/v2.1)|

### Przygotowanie wÅ‚asnego zbioru danych

JeÅ›li chcesz wytrenowaÄ‡ model na swoich wÅ‚asnych danych, wykonaj nastÄ™pujÄ…ce kroki:

**Krok 1:** Upewnij siÄ™, Å¼e Twoje dane sÄ… w formacie [Huggingface LeRobot V2.1](https://github.com/huggingface/lerobot). Struktura ÅºrÃ³dÅ‚owa powinna wyglÄ…daÄ‡ tak:

```
source_dir/
    â”œâ”€â”€ dataset1_name    # Pierwszy zbiÃ³r danych
    â”œâ”€â”€ dataset2_name    # Drugi zbiÃ³r danych
    â”œâ”€â”€ dataset3_name    # Trzeci zbiÃ³r danych
    â””â”€â”€ ...
```

**Krok 2:** Przekonwertuj zbiÃ³r danych do wymaganego formatu uÅ¼ywajÄ…c poniÅ¼szej komendy:

```bash
cd prepare_data
python prepare_training_data.py \
    --source_dir /Å›cieÅ¼ka/do/source_dir \
    --target_dir /Å›cieÅ¼ka/gdzie/zapisaÄ‡/przekonwertowane/dane \
    --dataset_name "dataset1_name" \
    --robot_name "tag robota w zbiorze danych"  # np. "Unitree Z1 Robot Arm" lub "Unitree G1 Robot with Gripper"
```

**WyjaÅ›nienie parametrÃ³w:**
- `--source_dir`: Å›cieÅ¼ka do katalogu z oryginalnymi danymi
- `--target_dir`: Å›cieÅ¼ka gdzie zapisaÄ‡ przetworzone dane
- `--dataset_name`: nazwa konkretnego zbioru danych do konwersji
- `--robot_name`: opisowa nazwa robota (uÅ¼ywana w metadanych)

**Krok 3:** Po konwersji, struktura danych bÄ™dzie wyglÄ…daÄ‡ nastÄ™pujÄ…co:

```
target_dir/
    â”œâ”€â”€ videos                      # Filmy z demonstracji
    â”‚     â”œâ”€â”€dataset1_name
    â”‚     â”‚   â”œâ”€â”€camera_view_dir    # Widok z konkretnej kamery
    â”‚     â”‚       â”œâ”€â”€ 0.mp4         # Film z pierwszej demonstracji
    â”‚     â”‚       â”œâ”€â”€ 1.mp4         # Film z drugiej demonstracji
    â”‚     â”‚       â””â”€â”€ ...
    â”‚     â””â”€â”€ ...
    â”œâ”€â”€ transitions                 # Dane o stanach i akcjach robota
    â”‚    â”œâ”€â”€ dataset1_name
    â”‚        â”œâ”€â”€ meta_data          # Metadane (normalizacja, statystyki)
    â”‚        â”œâ”€â”€ 0.h5               # Dane z pierwszej demonstracji
    â”‚        â”œâ”€â”€ 1.h5               # Dane z drugiej demonstracji
    â”‚        â””â”€â”€ ...
    â””â”€â”€  dataset1_name.csv          # Indeks wszystkich demonstracji
```

**WaÅ¼na uwaga:** Trening modelu obsÅ‚uguje tylko dane z jednej gÅ‚Ã³wnej kamery. JeÅ›li TwÃ³j zbiÃ³r danych zawiera wiele widokÃ³w kamer, usuÅ„ odpowiednie wartoÅ›ci z kolumny `data_dir` w pliku CSV.

## ğŸš´â€â™‚ï¸ Trening Modelu

**WyjaÅ›nienie:** Ta sekcja opisuje jak trenowaÄ‡ wÅ‚asny model. Proces treningu uczy model przewidywaÄ‡ ruchy robota na podstawie obrazÃ³w z kamery i instrukcji.

### A. Strategia Treningu

Nasz proces treningu skÅ‚ada siÄ™ z trzech etapÃ³w:

- **Krok 1: Pretrening modelu Å›wiata**
  
  Najpierw dostrajamy model generowania wideo jako model Å›wiata, uÅ¼ywajÄ…c duÅ¼ego zbioru danych [Open-X](https://robotics-transformer-x.github.io/). W tym etapie model uczy siÄ™ podstawowych zasad fizyki i interakcji z obiektami.

- **Krok 2: Trening w trybie podejmowania decyzji**
  
  NastÄ™pnie trenujemy $\text{UnifoLM-WMA}$ w trybie podejmowania decyzji na zbiorze danych specyficznym dla zadania. Model uczy siÄ™ jakie akcje wykonaÄ‡, aby osiÄ…gnÄ…Ä‡ cel.
  
  <div align="left">
      <img src="assets/pngs/dm_mode.png" width="600">
  </div>

- **Krok 3: Trening w trybie symulacji**
  
  Na koniec trenujemy $\text{UnifoLM-WMA}$ w trybie symulacji na tym samym zbiorze danych. Model uczy siÄ™ przewidywaÄ‡ efekty swoich akcji.
  
  <div align="left">
      <img src="assets/pngs/sim_mode.png" width="600">
  </div>

**Uwaga:** JeÅ›li potrzebujesz tylko jednego trybu dziaÅ‚ania $\text{UnifoLM-WMA}$, moÅ¼esz pominÄ…Ä‡ odpowiedni krok.

### B. Przeprowadzanie Treningu

Aby przeprowadziÄ‡ trening na jednym lub wielu zbiorach danych, wykonaj poniÅ¼sze kroki:

**Krok 1: Konfiguracja wymiarÃ³w**

DomyÅ›lnie zakÅ‚adamy maksymalnie 16 stopni swobody (DoF - Degrees of Freedom). Stopnie swobody to liczba niezaleÅ¼nych przegubÃ³w/osi ruchu robota.

JeÅ›li TwÃ³j robot ma wiÄ™cej niÅ¼ 16 DoF, zaktualizuj parametry `agent_state_dim` i `agent_action_dim` w pliku [configs/train/config.yaml](https://github.com/unitreerobotics/unifolm-wma/blob/working/configs/train/config.yaml).

**Krok 2: Konfiguracja ksztaÅ‚tÃ³w danych wejÅ›ciowych**

Ustaw rozmiary wejÅ›ciowe dla kaÅ¼dej modalnoÅ›ci (obraz, stan robota, akcja) w pliku [configs/train/meta.json](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/train/meta.json).

**Krok 3: Konfiguracja parametrÃ³w treningu**

Skonfiguruj parametry treningu w [configs/train/config.yaml](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/train/config.yaml):

```yaml
model:
    # ÅšcieÅ¼ka do pretrenowanego modelu (zalecamy uÅ¼ycie UnifoLM-WMA-0_Base)
    pretrained_checkpoint: /path/to/pretrained/checkpoint
    ...
    # Czy trenowaÄ‡ tylko w trybie podejmowania decyzji
    # True = tylko podejmowanie decyzji
    # False = jednoczeÅ›nie podejmowanie decyzji i symulacja
    decision_making_only: True
    ...
data:
    ...
    train:
        ...
        # ÅšcieÅ¼ka do katalogu z danymi treningowymi
        data_dir: /path/to/training/dataset/directory
    # Lista zbiorÃ³w danych i ich wagi (suma wag musi wynosiÄ‡ 1.0)
    # Wagi okreÅ›lajÄ… jak czÄ™sto dany zbiÃ³r danych jest uÅ¼ywany w treningu
    dataset_and_weights:
        dataset1_name: 0.2  # 20% danych treningowych
        dataset2_name: 0.2  # 20% danych treningowych
        dataset3_name: 0.2  # 20% danych treningowych
        dataset4_name: 0.2  # 20% danych treningowych
        dataset5_name: 0.2  # 20% danych treningowych
```

**Krok 4: Konfiguracja skryptu treningu**

Ustaw zmienne `experiment_name` (nazwa eksperymentu) i `save_root` (katalog gdzie zapisaÄ‡ model) w [scripts/train.sh](https://github.com/unitreerobotics/unitree-world-model/blob/main/scripts/train.sh).

**Krok 5: Uruchomienie treningu**

Rozpocznij trening wykonujÄ…c komendÄ™:

```bash
bash scripts/train.sh
```

**Co siÄ™ dzieje podczas treningu:**
- Model przetwarza kolejne partie danych (batches)
- Oblicza bÅ‚Ä™dy przewidywaÅ„ (loss)
- Aktualizuje wagi sieci neuronowej, aby poprawiÄ‡ przewidywania
- Regularnie zapisuje punkty kontrolne (checkpoints)
- MoÅ¼e to potrwaÄ‡ od kilku godzin do kilku dni, w zaleÅ¼noÅ›ci od rozmiaru danych i sprzÄ™tu

## ğŸŒ Inferencja w Interaktywnym Trybie Symulacji

**WyjaÅ›nienie:** Inferencja to proces uÅ¼ywania wytrenowanego modelu do generowania przewidywaÅ„. W trybie interaktywnej symulacji, model przewiduje przyszÅ‚e stany Å›rodowiska na podstawie obecnego stanu i planowanych akcji.

Aby uruchomiÄ‡ model Å›wiata w interaktywnym trybie symulacji, wykonaj nastÄ™pujÄ…ce kroki:

**Krok 1: Przygotowanie promptÃ³w (opcjonalnie)**

JeÅ›li chcesz przetestowaÄ‡ model na wÅ‚asnych danych, pomiÅ„ ten krok i uÅ¼yj dostarczonych przykÅ‚adÃ³w. W przeciwnym razie przygotuj wÅ‚asne prompty zgodnie z formatem w [examples/world_model_interaction_prompts](https://github.com/unitreerobotics/unitree-world-model/tree/main/examples/world_model_interaction_prompts):

```
world_model_interaction_prompts/
  â”œâ”€â”€ images                          # Obrazy jako punkty startowe
  â”‚    â”œâ”€â”€ dataset1_name
  â”‚    â”‚       â”œâ”€â”€ 0.png              # Obraz promptu (obecny stan sceny)
  â”‚    â”‚       â””â”€â”€ ...
  â”‚    â””â”€â”€ ...
  â”œâ”€â”€ transitions                     # Dane o stanach robota
  â”‚    â”œâ”€â”€ dataset1_name
  â”‚    â”‚       â”œâ”€â”€ meta_data          # UÅ¼ywane do normalizacji
  â”‚    â”‚       â”œâ”€â”€ 0.h5               # Stan robota odpowiadajÄ…cy obrazowi
  â”‚    â”‚       â”‚                      # (w trybie interakcji uÅ¼ywany tylko do 
  â”‚    â”‚       â”‚                      #  pobrania stanu robota)
  â”‚    â”‚       â””â”€â”€ ...
  â”‚    â””â”€â”€ ...
  â”œâ”€â”€  dataset1_name.csv              # Plik indeksujÄ…cy obrazy, instrukcje 
  â”‚                                   # tekstowe i odpowiadajÄ…ce stany robota
  â””â”€â”€ ...
```

**Krok 2: Konfiguracja Å›cieÅ¼ek**

OkreÅ›l poprawne Å›cieÅ¼ki dla `pretrained_checkpoint` (np. $\text{UnifoLM-WMA-0}_{Dual}$) i `data_dir` w [configs/inference/world_model_interaction.yaml](https://github.com/unitreerobotics/unitree-world-model/blob/main/configs/inference/world_model_interaction.yaml).

**Krok 3: Uruchomienie inferencji**

Ustaw Å›cieÅ¼ki dla `checkpoint`, `res_dir` (katalog wynikÃ³w) i `prompt_dir` (katalog z promptami) w [scripts/run_world_model_interaction.sh](https://github.com/unitreerobotics/unitree-world-model/blob/main/scripts/run_world_model_interaction.sh). 

OkreÅ›l wszystkie nazwy zbiorÃ³w danych w `datasets=(...)`, a nastÄ™pnie uruchom inferencjÄ™ komendÄ…:

```bash
bash scripts/run_world_model_interaction.sh
```

**Co siÄ™ dzieje:**
- Model wczytuje obraz poczÄ…tkowy i instrukcjÄ™
- Generuje sekwencjÄ™ przyszÅ‚ych obrazÃ³w pokazujÄ…cÄ… jak robot wykona zadanie
- Wyniki (wygenerowane wideo) sÄ… zapisywane w katalogu `res_dir`

## ğŸ§  Inferencja i WdroÅ¼enie w Trybie Podejmowania Decyzji

**WyjaÅ›nienie:** W tym trybie uruchamiamy model na rzeczywistym robocie. Model dziaÅ‚a na serwerze (mocny komputer z GPU), a robot (klient) wysyÅ‚a obserwacje i otrzymuje akcje do wykonania. To architektura klient-serwer, ktÃ³ra pozwala na wykorzystanie mocy obliczeniowej serwera do sterowania robotem w czasie rzeczywistym.

### Konfiguracja Serwera

Serwer to komputer, ktÃ³ry uruchamia model i oblicza jakie akcje robot powinien wykonaÄ‡.

**Krok 1: Konfiguracja parametrÃ³w serwera**

OkreÅ›l zmienne `ckpt` (Å›cieÅ¼ka do modelu), `res_dir` (katalog wynikÃ³w), `datasets` (lista zbiorÃ³w danych) w [scripts/run_real_eval_server.sh](https://github.com/unitreerobotics/unifolm-world-model-action/blob/main/scripts/run_real_eval_server.sh).

**Krok 2: Konfiguracja danych**

Skonfiguruj `data_dir` (katalog danych) i `dataset_and_weights` (wagi zbiorÃ³w) w [config/inference/world_model_decision_making.yaml](https://github.com/unitreerobotics/unifolm-world-model-action/blob/f12b4782652ca00452941d851b17446e4ee7124a/configs/inference/world_model_decision_making.yaml#L225).

**Krok 3: Uruchomienie serwera**

Uruchom serwer uÅ¼ywajÄ…c nastÄ™pujÄ…cych komend:

```bash
# Aktywuj Å›rodowisko conda
conda activate unifolm-wma

# PrzejdÅº do katalogu projektu
cd unifolm-world-model-action

# Uruchom serwer (bÄ™dzie nasÅ‚uchiwaÅ‚ na poÅ‚Ä…czenia od klienta)
bash scripts/run_real_eval_server.sh
```

Serwer jest teraz uruchomiony i czeka na poÅ‚Ä…czenie od klienta robota.

### Konfiguracja Klienta (Robot)

Klient to robot, ktÃ³ry bÄ™dzie wykonywaÅ‚ akcje przewidziane przez model na serwerze.

**Krok 1: Przygotowanie Å›rodowiska robota**

PostÄ™puj zgodnie z instrukcjami w [unitree_deploy/README.md](https://github.com/unitreerobotics/unifolm-world-model-action/blob/main/unitree_deploy/README.md), aby:
- UtworzyÄ‡ Å›rodowisko conda `unitree_deploy`
- ZainstalowaÄ‡ wymagane pakiety
- UruchomiÄ‡ kontrolery lub usÅ‚ugi na rzeczywistym robocie

**Krok 2: Ustanowienie tunelu SSH**

OtwÃ³rz nowy terminal i ustanÃ³w tunel SSH od klienta do serwera. To pozwala robotowi bezpiecznie komunikowaÄ‡ siÄ™ z serwerem:

```bash
# ZastÄ…p user_name i remote_server_IP swoimi danymi
ssh user_name@remote_server_IP -CNg -L 8000:127.0.0.1:8000
```

**WyjaÅ›nienie tunelu:**
- `-C`: kompresja danych (szybsza transmisja)
- `-N`: nie wykonuj Å¼adnych komend zdalnych
- `-g`: pozwÃ³l na poÅ‚Ä…czenia zdalne
- `-L 8000:127.0.0.1:8000`: przekieruj port lokalny 8000 na port 8000 serwera

**Krok 3: Uruchomienie klienta robota**

W osobnym terminalu uruchom skrypt klienta robota:

```bash
cd unitree_deploy
python scripts/robot_client.py \
    --robot_type "g1_dex1" \              # Typ robota
    --action_horizon 16 \                 # Ile akcji przewidywaÄ‡ do przodu
    --exe_steps 16 \                      # Ile krokÃ³w wykonaÄ‡
    --observation_horizon 2 \             # Ile obserwacji uwzglÄ™dniÄ‡
    --language_instruction "pack black camera into box" \  # Instrukcja zadania
    --output_dir ./results \              # Katalog wynikÃ³w
    --control_freq 15                     # CzÄ™stotliwoÅ›Ä‡ sterowania (Hz)
```

**WyjaÅ›nienie parametrÃ³w:**
- `--robot_type`: okreÅ›la jakiego robota uÅ¼ywasz (g1_dex1, z1_realsense, z1_dual_dex1_realsense)
- `--action_horizon`: ile krokÃ³w do przodu model powinien planowaÄ‡
- `--exe_steps`: ile krokÃ³w faktycznie wykonaÄ‡ (zazwyczaj rÃ³wne action_horizon)
- `--observation_horizon`: ile poprzednich obserwacji uwzglÄ™dniÄ‡ w decyzji
- `--language_instruction`: opisuje zadanie, ktÃ³re robot ma wykonaÄ‡
- `--control_freq`: jak czÄ™sto wysyÅ‚aÄ‡ komendy do robota (w Hz)

**Co siÄ™ dzieje:**
1. Robot zbiera obserwacje (obrazy z kamery, pozycje przegubÃ³w)
2. WysyÅ‚a je do serwera przez tunel SSH
3. Serwer uÅ¼ywa modelu do obliczenia najlepszych akcji
4. Robot otrzymuje akcje i wykonuje je
5. Proces powtarza siÄ™ w pÄ™tli, aÅ¼ zadanie zostanie ukoÅ„czone

## ğŸ“ Architektura Kodu

**WyjaÅ›nienie:** Ta sekcja opisuje organizacjÄ™ plikÃ³w w projekcie. Zrozumienie struktury pomoÅ¼e Ci znaleÅºÄ‡ odpowiednie pliki, gdy bÄ™dziesz chciaÅ‚ coÅ› zmodyfikowaÄ‡.

Oto przeglÄ…d struktury kodu projektu i gÅ‚Ã³wnych komponentÃ³w:

```
unifolm-world-model-action/
    â”œâ”€â”€ assets/                         # Zasoby medialne
    â”‚   â”œâ”€â”€ gifs/                       # Animacje demonstracyjne
    â”‚   â””â”€â”€ pngs/                       # Obrazy i diagramy
    â”‚
    â”œâ”€â”€ configs/                        # Pliki konfiguracyjne
    â”‚   â”œâ”€â”€ inference/                  # Konfiguracja dla inferencji
    â”‚   â”‚   â”œâ”€â”€ world_model_interaction.yaml         # Tryb symulacji
    â”‚   â”‚   â””â”€â”€ world_model_decision_making.yaml     # Tryb podejmowania decyzji
    â”‚   â””â”€â”€ train/                      # Konfiguracja dla treningu
    â”‚       â”œâ”€â”€ config.yaml             # GÅ‚Ã³wne parametry treningu
    â”‚       â””â”€â”€ meta.json               # Metadane zbiorÃ³w danych
    â”‚
    â”œâ”€â”€ examples/                       # PrzykÅ‚adowe dane wejÅ›ciowe
    â”‚   â””â”€â”€ world_model_interaction_prompts/  # PrzykÅ‚ady do trybu interakcji
    â”‚
    â”œâ”€â”€ external/                       # ZewnÄ™trzne pakiety i zaleÅ¼noÅ›ci
    â”‚   â””â”€â”€ dlimp/                      # Biblioteka pomocnicza
    â”‚
    â”œâ”€â”€ prepare_data/                   # Skrypty do przetwarzania danych
    â”‚   â””â”€â”€ prepare_training_data.py    # Konwersja danych do formatu treningowego
    â”‚
    â”œâ”€â”€ scripts/                        # GÅ‚Ã³wne skrypty projektu
    â”‚   â”œâ”€â”€ trainer.py                  # Skrypt treningu modelu
    â”‚   â”œâ”€â”€ train.sh                    # Skrypt bash do uruchomienia treningu
    â”‚   â”œâ”€â”€ evaluation/                 # Skrypty ewaluacji
    â”‚   â”‚   â”œâ”€â”€ real_eval_server.py     # Serwer dla rzeczywistego robota
    â”‚   â”‚   â”œâ”€â”€ world_model_interaction.py  # Interaktywna symulacja
    â”‚   â”‚   â””â”€â”€ eval_utils.py           # Funkcje pomocnicze
    â”‚   â”œâ”€â”€ run_real_eval_server.sh     # Uruchamia serwer ewaluacji
    â”‚   â””â”€â”€ run_world_model_interaction.sh  # Uruchamia tryb interakcji
    â”‚
    â”œâ”€â”€ src/                            # Kod ÅºrÃ³dÅ‚owy gÅ‚Ã³wnego pakietu
    â”‚   â””â”€â”€ unifolm_wma/                # Pakiet Python modelu Å›wiata
    â”‚       â”œâ”€â”€ data/                   # Åadowanie i przetwarzanie danych
    â”‚       â”‚   â”œâ”€â”€ dataloader.py       # Klasy do Å‚adowania danych
    â”‚       â”‚   â””â”€â”€ transforms.py       # Transformacje danych
    â”‚       â”œâ”€â”€ models/                 # Architektury modeli
    â”‚       â”‚   â”œâ”€â”€ world_model.py      # Model Å›wiata
    â”‚       â”‚   â””â”€â”€ action_head.py      # GÅ‚owa akcji (policy)
    â”‚       â”œâ”€â”€ modules/                # Niestandardowe moduÅ‚y
    â”‚       â”‚   â”œâ”€â”€ attention.py        # Mechanizmy uwagi
    â”‚       â”‚   â””â”€â”€ encoders.py         # Enkodery obrazÃ³w i stanÃ³w
    â”‚       â””â”€â”€ utils/                  # Funkcje pomocnicze
    â”‚           â”œâ”€â”€ visualization.py    # Wizualizacja wynikÃ³w
    â”‚           â””â”€â”€ training_utils.py   # NarzÄ™dzia do treningu
    â”‚
    â””â”€â”€ unitree_deploy/                 # Kod wdroÅ¼eniowy dla robotÃ³w Unitree
        â”œâ”€â”€ README.md                   # Dokumentacja wdroÅ¼enia
        â”œâ”€â”€ docs/                       # SzczegÃ³Å‚owa dokumentacja
        â”‚   â”œâ”€â”€ GettingStarted.md       # Przewodnik startowy
        â”‚   â”œâ”€â”€ build_robot.md          # Jak zbudowaÄ‡ konfiguracjÄ™ robota
        â”‚   â”œâ”€â”€ add_robot_arm.md        # Dodawanie nowych ramion
        â”‚   â”œâ”€â”€ add_robot_camera.md     # Dodawanie kamer
        â”‚   â””â”€â”€ add_robot_endeffector.md # Dodawanie chwytakÃ³w
        â”œâ”€â”€ scripts/                    # Skrypty wdroÅ¼eniowe
        â”‚   â””â”€â”€ robot_client.py         # Klient robota (sterowanie)
        â””â”€â”€ unitree_deploy/             # GÅ‚Ã³wny pakiet wdroÅ¼eniowy
            â”œâ”€â”€ robot/                  # Klasy robotÃ³w
            â”‚   â”œâ”€â”€ robot.py            # GÅ‚Ã³wna klasa robota
            â”‚   â”œâ”€â”€ robot_configs.py    # Konfiguracje robotÃ³w
            â”‚   â””â”€â”€ robot_utils.py      # Funkcje pomocnicze
            â”œâ”€â”€ robot_devices/          # Komponenty robota
            â”‚   â”œâ”€â”€ arm/                # Ramiona robotÃ³w
            â”‚   â”‚   â”œâ”€â”€ z1_arm.py       # RamiÄ™ Z1
            â”‚   â”‚   â”œâ”€â”€ g1_arm.py       # RamiÄ™ G1
            â”‚   â”‚   â””â”€â”€ configs.py      # Konfiguracje ramion
            â”‚   â”œâ”€â”€ cameras/            # Kamery
            â”‚   â”‚   â””â”€â”€ realsense.py    # Kamera RealSense
            â”‚   â””â”€â”€ endeffector/        # Efektory koÅ„cowe (chwytaki)
            â”‚       â””â”€â”€ dex1.py         # Chwytka Dex1
            â”œâ”€â”€ utils/                  # NarzÄ™dzia pomocnicze
            â”‚   â”œâ”€â”€ trajectory_generator.py  # Generowanie trajektorii
            â”‚   â”œâ”€â”€ eval_utils.py       # Ewaluacja na robocie
            â”‚   â””â”€â”€ visualizer.py       # Wizualizacja danych robota
            â””â”€â”€ real_unitree_env.py     # Åšrodowisko rzeczywistego robota
```

**NajwaÅ¼niejsze pliki dla poczÄ…tkujÄ…cych:**
- `README.md` (ten plik) - dokumentacja projektu
- `configs/` - tu zmieniasz parametry treningu i inferencji
- `unitree_deploy/scripts/robot_client.py` - tu steruj robotem
- `scripts/train.sh` - tu uruchamiasz trening

## ğŸ™ PodziÄ™kowania

DuÅ¼a czÄ™Å›Ä‡ kodu pochodzi z nastÄ™pujÄ…cych projektÃ³w open-source:
- [DynamiCrafter](https://github.com/Doubiiu/DynamiCrafter) - generowanie wideo
- [Diffusion Policy](https://github.com/real-stanford/diffusion_policy) - polityki dyfuzyjne
- [ACT](https://github.com/MarkFzp/act-plus-plus) - Action Chunking Transformer
- [HPT](https://github.com/liruiw/HPT) - Heterogeneous Pre-trained Transformers

DziÄ™kujemy autorom za udostÄ™pnienie swojego kodu!

## ğŸ“ Cytowanie

JeÅ›li uÅ¼ywasz tego projektu w swojej pracy naukowej, prosimy o cytowanie:

```
@misc{unifolm-wma-0,
  author       = {Unitree},
  title        = {UnifoLM-WMA-0: A World-Model-Action (WMA) Framework under UnifoLM Family},
  year         = {2025},
}
```

## ğŸ§ª Testowanie

Projekt zawiera testy jednostkowe do weryfikacji poprawnoÅ›ci instalacji i konfiguracji.

### Uruchomienie testÃ³w

```bash
# Zainstaluj zaleÅ¼noÅ›ci testowe
pip install -e ".[test]"

# Uruchom wszystkie testy
pytest

# Uruchom testy z raportem pokrycia kodu
pytest --cov=unifolm_wma --cov-report=html
```

WiÄ™cej informacji o testach znajduje siÄ™ w [tests/README.md](tests/README.md).

## ğŸ“š Dodatkowe Zasoby dla PoczÄ…tkujÄ…cych

**Gdzie szukaÄ‡ pomocy:**
- [Dokumentacja Unitree Deploy](unitree_deploy/README.md) - szczegÃ³Å‚y wdroÅ¼enia
- [Dokumentacja wideo LeRobot](https://github.com/huggingface/lerobot) - format danych
- [Forum Unitree](https://www.unitree.com/) - wsparcie spoÅ‚ecznoÅ›ci

**Podstawowe pojÄ™cia:**
- **DoF (Degrees of Freedom)**: Liczba niezaleÅ¼nych ruchÃ³w, ktÃ³re robot moÅ¼e wykonaÄ‡
- **Checkpoint**: Zapisany stan modelu po treningu
- **Inference**: UÅ¼ywanie wytrenowanego modelu do przewidywaÅ„
- **Policy**: Strategia decyzyjna robota (co robiÄ‡ w danej sytuacji)
- **World Model**: Model przewidujÄ…cy jak zmieni siÄ™ Å›wiat po akcjach robota
